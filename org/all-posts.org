#+hugo_base_dir: ../
#+seq_todo: TODO DRAFT DONE
#+property: header-args :eval never-export
#+startup: indent

#+author: Marco Ribeiro

* Extração de Dados do Amazon DynamoDB para Postgres/MySQL :postgres:mysql:dynamodb:python:etl:
:PROPERTIES:
:EXPORT_FILE_NAME: extracao-dados-dynamodb-postgres-mysql
:EXPORT_DATE: 2022-10-12
:END:

Um tempo atrás criei uma aplicação web para gerenciamento de contas a pagar e a receber utilizando a infraestrutura da AWS e o Amplify. Ao se criar uma API GraphQL utilizando o Amplify CLI, toda a infraestrutura necessária é criada, incluindo funções Lambda e a base de dados DynamoDB. É tudo muito lindo, mas essa praticidade tem um custo, e não poder executar consultas SQL diretamente nos dados dificulta bastante a análise de dados.

Para solucionar esse problema e cruzar os dados de contas a pagar com os de vendas, que são vem de um sistema com banco de dados Microsoft SQL, a solução mais fácil foi extrair as tabelas do DynamoDB e salvá-los em uma base Postgres (Ou qualquer banco de dados relacional que você preferir). Para isso foram utilizados os pacotes abaixo:


** Importação dos pacotes necessários
#+begin_src python
import boto3
import pandas as pd
from sqlalchemy import create_engine
#+end_src


** Criação dos objetos utilizados na extração
#+begin_src python

engine = create_engine('postgresql://nome_usuario:senha@endereco_banco:5432/nome_base')
client = boto3.client('dynamodb')
dynamodb = boto3.resource('dynamodb', region_name="us-east-1")

# Listar tabelas
client.list_tables()

#+end_src

** Realização do leitura completa da tabela

Para esse passo foi utilizada uma função que executa o full scan da tabela. Como cada consulta tem um limite de registros retornados, é utilizada a chave 'LastEvaluatedKey', retornada a cada consulta, para solicitar do próximo batch em um loop.

#+begin_src python
def table_scan(table):
    response = table.scan()
    data = response['Items']

    while 'LastEvaluatedKey' in response:
        response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])
        data.extend(response['Items'])

    return data


table = dynamodb.Table('parcelas-xxxxstringaleatoriaxxxxx-prod')
parcelas = table_scan(table)
#+end_src

** Criação do dataframe, tratamento e gravação dos dados

Nessa etapa é criado um dataframe Pandas para o tratamento inicial dos dados e a utilizado o método 'to_sql', que utiliza a engine do sqlalchemy criada anteriormente, para a gravação dos dados do DynamoDB na tabela SQL.

#+begin_src python

# Cria um dataframe pandas com os dados da tabela
df = pd.DataFrame(contas)

# Converte os dados da colunas para o tipo desejado
df = df.astype({
    'valor':'float',
    'numeroParcela':'int',
    'quantidadeParcelas':'int'
})

# Remove colunas
df.drop(labels='__typename', axis=1, inplace=True, errors='raise')

# Grava dataframe no banco de dados
df.to_sql('parcelas', con=engine, if_exists='replace', index=False, schema='contas')

#+end_src

É isso, com os dados salvos em um banco de dados relacional é possível agora realizar consultas SQL e/ou utilizar o DBT para criar toda sorte de views e tabelas automaticamente para facilitar a análise de dados e a criação de dashboards.


* Refatoração Azure Data Factory com Bash Script :datafactory:azure:sed:jq:bash:script:
:PROPERTIES:
:EXPORT_FILE_NAME: refatoracao-data-factory-bash
:EXPORT_DATE: 2022-10-06
:END:

Nem tudo na vida de um engenheiro de dados são flores. As vezes é necessário fazer a "difícil" escolha entre usar força bruta ou passar horas alterando manualmente pipelines de dados em interfaces ‘clica e arrasta’. Para ilustrar o problema em questão, usaremos o caso do orquestrador de pipelines de dados Azure Data Factory da Microsoft. Por trás da interface gráfica, os pipelines, linked services e data sources são salvos em arquivos JSON que se referenciam e que posteriormente são processados para a geração dos ARM Templates e implantação nos workspaces.

Para esse trabalho os principais comandos utilizados foram __sed__ para a substituição de strings e __jq__ para a leitura da estrutura dos arquivos JSON diretamente no terminal. Os arquivos do Data Factory foram clonados do repositório GIT do projeto e seguintes comandos foram utilizados para a realização das alterações necessárias.

** Renomear artefatos no arquivo JSON
Para renomear um pipeline ou linked service, não basta alterar o nome do arquivo ou o seu conteúdo, é necessário fazer os dois. Por isso, para alterar o nome de diversos artefatos para, por exemplo, remover a referencia ao ambiente do nome do objeto e não ter problemas quanto o pipeline de DevOps fizer o deploy do ARM Template em um ambiente diferente.

#+begin_src bash

# Remover sufixo referente ao ambiente dos pipelines
sed -i 's/_prd//g' *.json

# Renomear arquivos json das pipelines de acordo com novo nome
for i in *.json; do new=$(jq -r '.name' "${i}").json && mv "$i" "$new"; done

#+end_src

** Alterar Path dos notebooks de PRD para novo path genérico

#+begin_src bash
sed -i 's/"name": "ProjetoX_PRD/"name": "ProjetoX/g' *.json
#+end_src

** Alterar linked services com referencia ao integration runtime

#+begin_src bash

sed -i 's/"referenceName": "IR-XPTO-PRD"/"referenceName": "ir-xpto"/g' *.json

#+end_src

** Alterar Path dos notebooks do Databricks para nova estrutura

Suponhamos que os notebooks do databricks foram desenvolvidos nas pastas do workspace e posteriormente foi feita a integração de um repositório git. O path necessariamente terá que ser alterado em todos os pipelines que utilizem a atividade do databricks e isso pode ser feito em uma linha utilizando o comando __sed__.

#+begin_src bash
sed -i 's/"notebookPath": "\/path_antigo\/pasta_qualquer/"notebookPath": "/g' *.json
#+end_src

** Consolidar diversos linked services em um

#+begin_src bash
linked_services=("lks_abc1" "lks_abc_prd" "lks_abc_test" "lks_abc_generic")
for s in "${linked_services[@]}"; do
	sed -i 's/"referenceName": "'$s'"/"referenceName": "lks_abc"/g' *.json
done
#+end_src

** Remover datasets sem objetos vinculados

#+begin_src bash
datasets=$(ls dataset | sed 's/\(.*\)\..*/\1/')
for f in $datasets; do
        pipelines=$(grep -l -R '"referenceName": "'$f'",' pipeline/* | sed 's/.*\///;s/\(.*\)\..*/\1/')
        lenght=$(echo -n "$pipelines" | grep -c '^')

        if [ $lenght -eq 0 ]; then
        	rm datafactory/dataset/$f.json
        fi
done

#+end_src

* VIM
:PROPERTIES:
:EXPORT_FILE_NAME: vim-key-bindings
:EXPORT_DATE: 2020-01-10
:EXPORT_HUGO_BLACKFRIDAY: :extensions hardLineBreak
:EXPORT_TITLE: Vim Key Bindings
:END:
Vim Command Language and examples of how to get things done.

*** Verbs
#+BEGIN_SRC
v : visual
c : change
d : delete
y : yank/copy
#+END_SRC

*** Modifiers
#+BEGIN_SRC
i : inside
a : around
t : till..finds a character
f : find..like till except including the char
/ : search..find a string/regex
#+END_SRC

*** Text Objects
#+BEGIN_SRC 
w : word
s : sentence
p : paragraph
b : block/parentheses,
t : tag, works for html/xml
#+END_SRC


** VIM Shortcuts
*** Navigation
#+BEGIN_SRC 
zz  : current line to top
zt  : current line to middle
zb  : current line to bottom

C+y : scroll up one line
C+e : scroll down one line
C+u : scroll up half-page
C+d : scroll down half-page
C+b : scroll up full-page
C+f : scroll down full-page

H   : cursor to the top of the screen
M   : cursor to the middle of the screen
L   : cursor to the botton of the screen

e   : go to the end of the current word.
E   : go to the end of the current WORD.
b   : go to the previous (before) word.
B   : go to the previous (before) WORD.
w   : go to the next word.
W   : go to the next WORD.
g;  : go to the last editing place.

0   : beginning of line
^   : first char of line
$   : end of line

{   : Go to the beginning of the current paragraph.
}   : Go to the end of the current paragraph.

/i  : Search to the next occurrence of it.
?i  : Search to the previous occurrence of it.
–   : Go to the next occurrence of the current word under the cursor.
#   : Go to the previous occurrence of the current word under the cursor.

%   : Go to the matching braces, or parenthesis inside code.
#+END_SRC

*** Folding
#+BEGIN_SRC
zf#j      : creates a fold from the cursor down # lines.
zf/string : creates a fold from the cursor to string .
za        : opens/closes all folds
zj        : moves the cursor to the next fold
zk        : moves the cursor to the previous fold
zc        : closes a fold at the cursor
zo        : opens a fold at the cursor
zO        : opens all folds at the cursor
zm        : increases the foldlevel by one
zM        : closes all open folds
zr        : decreases the foldlevel by one
zR        : decreases the foldlevel to zero — all folds will be open
zd        : deletes the fold at the cursor
zE        : deletes all folds
[z        : move to start of open fold
]z        : move to end of open fold
#+END_SRC

*** Jump
#+BEGIN_SRC
C+o : go to origin
#+END_SRC

*** Marks
#+BEGIN_SRC
m + letter : create mark
' + letter : go to mark
#+END_SRC

*** Yank / Paste
#+BEGIN_SRC
:reg : list registers
"1p  : paste previous deletes/yanks

yiw  : Yank inner word (copy word under cursor, say "first").
viwp : Select "second", then replace it with "first".

yi(  : equivalent to yib, yanks the contents inside parenthesis.
yi{  : equivalent to yiB, yanks the contents inside braces.
#+END_SRC

*** Insert Multiple
#+BEGIN_SRC
80i/<ESC>    : insert 80 '/' in a line
20Otest<ESC> : insert 20 lines of 'test'
#+END_SRC

*** Change Multiple
#+BEGIN_SRC
 *      : to select all words
cgn+Esc : to change de word
.       : to change the next

Esc     : to enter 'command mode'
C-v     : to enter visual block mode and select the columns of text.
M-i     : to enter insert mode and type the text you want to insert.
Esc     : wait 1 second and the inserted text will appear on every line.

:%s/foo/bar/g
:s/foo/bar/g
:%s/foo/bar/gc
#+END_SRC

*** Selection
#+BEGIN_SRC
ggVG : select all
#+END_SRC

*** Letter Case
#+BEGIN_SRC
~    : changes the case of current character
guu  : change current line from upper to lower.
gUU  : change current LINE from lower to upper.
guw  : change to end of current WORD from upper to lower.
guaw : change all of current WORD to lower.
gUw  : change to end of current WORD from lower to upper.
gUaw : change all of current WORD to upper.
g~~  : invert case to entire line
guG  : change to lowercase until the end of document.
#+END_SRC

*** Split
#+BEGIN_SRC
:sp           : horizontal split
:vs           : vertical split

C+W S         : horizontal splitting
C+W v         : vertical splitting
C+W q         : close split
C+W C+W       : switch between windows
C+W J K, H, L : switch to adjacent window ( up, down, left, right )

C-w t C-w K   : change two vertically split windows to horizonally split
C-w t C-w H   : Horizontally to vertically

C-w t         : makes the first (topleft) window current
C-w K         : moves the current window to full-width at the very top
C-w H         : moves the current window to full-height at far left
#+END_SRC

*** Tags
*** Spaces
#+BEGIN_SRC
:%le	: Remove left spaces in the entire file
#+END_SRC

*** Macros
#+BEGIN_SRC
qd : start recording to register d
q  : stop recording
@d : execute your macro
@@ : execute your macro again
#+END_SRC


** VIM Plugins
*** Prettier
#+BEGIN_SRC
L-p : Manually trigger Prettier
#+END_SRC

*** FZF
#+BEGIN_SRC
C-t : tab split
C-x : split
C-v : vsplit
#+END_SRC

*** Surround
#+BEGIN_SRC
cs"'  : replace " with '
cst"  : replace tag with "
ds"   : to remove " delimiters entirely.
ysiw] : add brackets arround a word.
cs]{  : add some space (use } instead of { for no space): cs]{
yssb  : wrap the entire line in parentheses with yssb or yss).
#+END_SRC

*** Commenter
#+BEGIN_SRC
L-cc       : Comment out the current line or text selected in visual mode.
L-cn       : Same as cc but forces nesting.
L-c<space> : Toggles the comment state of the selected line(s).
L-cm       : Comments the given lines using only one set of multipart delimiters.
L-ci       : Toggles the comment state of the selected line(s) individually.
L-cs       : Comments out the selected lines with a pretty block formatted layout.
L-cy       : Same as cc except that the commented line(s) are yanked first.
L-c$       : Comments the current line from the cursor to the end of line.
L-cA       : Adds comment delimiters to the end of line and goes into insert mode between them.
L-ca       : Switches to the alternative set of delimiters.
L-cl       : Delimiters are aligned down the left side (<leader>cl) or both sides (<leader>cb).
L-cu       : Uncomments the selected line(s).
#+END_SRC

*** vim-easy-align
#+BEGIN_SRC
vipga=  : visual-select inner paragraph, Start EasyAlign (ga), Align around =
gaip=   : Start EasyAlign for inner paragraph, Align around =
=       : Around the 1st occurrences
2=      : Around the 2nd occurrences
*=      : Around all occurrences
**=     : Left/Right alternating alignment around all occurrences
<Enter> : Switching between left/right/center alignment modes
#+END_SRC

*** splitjoin.vim
#+BEGIN_SRC
gS : to split a one-liner into multiple lines
gJ : to join a block into a single-line statement.
#+END_SRC

* Emacs
:PROPERTIES:
:EXPORT_FILE_NAME: emacs-key-bindings
:EXPORT_DATE: 2020-01-10
:EXPORT_HUGO_BLACKFRIDAY: :extensions hardLineBreak
:EXPORT_TITLE: Emacs Key Bindings
:END:
** Emacs Key Biddings
1. C-x reserved for Emacs native essential keybindings: buffer, window, frame, file, directory, etc…
2. C-c reserved for user and major mode:
   - C-c letter reserved for user. <F5>-<F9> reserved for user.
   - C-c C-letter reserved for major mode.
3. Don't rebind C-g, C-h and ESC.

4. Examples:

   - C-x C-f	find-file: first prompts for a filename and then loads that file into a editor buffer of the same name
   - C-x C-s	save-buffer: saves the buffer into the associated filename
   - C-x C-w	write-named-file: prompts for a new filename and writes the buffer into it
   - C-x b      switch-to-buffer: display a different buffer on the screen
   - C-x o      other-window: move the cursor to the other window
   - C-x C-b	list-buffers: lists those buffers currently loaded into emacs
   - C-g        to cancel any action you have started.

** Doom Emacs Key Bindings

- SPC h b b    Abre lista de atalhos de teclado
- M-d          Cria multiplos cursores
- SPC h v      Able lista de variaveis

*** evil-snipe :navigation:
f or t
; - next
, - previous

*** avy :navigation:
g s SPC - Começa a busca no buffer pelo avy
*** evil-lion
glip=  # gl - left align operator, ip - text paragraph, = separator
gLip,  # gL - right align operator, ',' separator

*** Org Mode

[[https://orgmode.org/orgcard.txt][Org Mode Key Bindings]]

C-M-i      (complete-symbol) complete word at point
C-c '      for editing the current code block.

**** [[https://orgmode.org/manual/setting-tags.html][setting tags]]

C-c C-q     (org-set-tags-command) enter new tags for the current headline.

**** [[https://www.gnu.org/software/emacs/manual/html_node/org/handling-links.html][handling links]]

C-c C-l     (org-insert-link) insert a link.

**** [[https://orgmode.org/manual/property-syntax.html][property syntax]]

C-c C-x p     (org-set-property) set a property.
C-c C-c s     (org-set-property) set a property in the current entry.
C-c C-c d     (org-delete-property) remove a property from the current entry.

**** [[https://orgmode.org/manual/structure-editing.html][structure editing]]

C-c C-x C-w     (org-cut-subtree) kill subtree.
C-c C-x M-w     (org-copy-subtree) copy subtree to kill ring.
C-c C-x C-y     (org-paste-subtree) yank subtree from kill ring.
C-c C-x c       (org-clone-subtree-with-time-shift) Clone a subtree by making a number of sibling copies of it.
C-c C-w         (org-refile) Refile entry or region to a different location.

** doom emacs links
[[https://noelwelsh.com/posts/2019-01-10-doom-emacs.html][doom emacs workflows]]
[[https://medium.com/urbint-engineering/emacs-doom-for-newbies-1f8038604e3b][emacs doom for newbies]]

** ox-hugo

C-c C-x p      Set property
SPC m o        Set property
C-c C-e H H    Export
C-c C-e H A    Export all
[[https://github.com/hlissner/doom-emacs/blob/develop/modules/config/default/+evil-bindings.el][
Exemplo all-posts]]

** Org Mode Code Snippets
Para adicionar um Code Snippet digite <s + TAB para autocompletar.

Para que o print() funcione em códigos python, adicionar o argumento no header ':results output'.

*** Exemplo Closure Javascript

#+BEGIN_SRC js :results output
const counter = () => {
    let n = 0;
    return {
        count: () => n++,
        reset: () => { n = 0; }
    };
}

const c = counter(), d = counter(); // Create two counters
console.log(c.count()) // => 0
console.log(d.count()) // => 0: they count independently
console.log(c.reset()) // reset() and count() methods share state
console.log(c.count()) // => 0: because we reset c
console.log(d.count()) // => 1: d was not reset
#+END_SRC

#+RESULTS:
: 0
: 0
: undefined
: 0
: 1

*** Exemplo Python

#+BEGIN_SRC python :results output
def hamming(a, b):
    return len([i for i in filter(lambda x: x[0] != x[1], zip(a, b))])

a = "Teste string 1"
b = "Teste string 2"

result = hamming(a, b)
print(result)
#+END_SRC

#+RESULTS:
: 1

#+BEGIN_SRC python :results output
a = [1, 2, 3]
b = ['a', 'b', 'c']

print(list(zip(a, b)))
#+END_SRC

#+RESULTS:
: [(1, 'a'), (2, 'b'), (3, 'c')]

* Hadoop
** Stanford Course
[[http://web.stanford.edu/class/cs246/][CS246: Mining Massive Data Sets]]
[[http://web.stanford.edu/class/cs246h/][CS246H - Mining Massive Data Sets: Hadoop Labs]]
[[https://www.youtube.com/playlist?list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV][CS246 Youtube playlist]]

* Criando Um Gerador de Imagens de Placas de Carros
:PROPERTIES:
:EXPORT_FILE_NAME: gerador-imagens-placas-carros
:END:
Para treinar um modelo de machine learning que realize o reconhecimento de
caracteres, OCR, é necessário uma grande quantidade de imagens de placas. Daria
um trabalho enorme conseguir milhares de imagens de carros com as placas
visiveis, recortar as placas das imagens e anotar as imagens com os textos dos
números das placas. Minha solução para obter essas milhares de imagens foi
simplesmente gerá-las.

Como o Brasil está adotando um novo padrão de placas é necessário utilizar dois
templates. O primeiro utiliza imagens de placas antigas e insere os caracteres
no padrão ABC-1234, o segundo com imagens de placas novas, insere os caracteres
com a nova fonte no padrão ABC1D23 com 4 letras e 3 números.

Para gerar os números de placas antigas foram utilizadas as seguintes funções:

#+BEGIN_SRC python :results output :exports both
from random import randrange

letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ";
letters_len = len(letters)
def get_idx(l): return randrange(l)

def plate_number_generator():
  plate_number = ''
  for i in range(3):
    plate_number += letters[get_idx(letters_len)]
  plate_number += format(randrange(9999), '04d')
  return plate_number

def new_plate_number_generator():
  plate_number = ''
  for i in range(3):
    plate_number += letters[get_idx(letters_len)]
  plate_number += format(randrange(9), '01d')
  plate_number += letters[get_idx(letters_len)]
  plate_number += format(randrange(99), '02d')
  return plate_number

plate = plate_number_generator()
new_plate = new_plate_number_generator()
print(f'Placa antiga: {plate}')
print(f'Placa nova: {new_plate}')
#+END_SRC

Para gerar as imagens das placas

* Como Substituir a Última Camada de Rede Neural no PyTorch
:PROPERTIES:
:EXPORT_FILE_NAME: substituir-ultima-camada-rede-neural-pytorch
:EXPORT_DATE: 2020-01-14
:END:

Durante o desenvolvimento de uma aplicação utilizando /machine learning/ para,
por exemplo, classificar imagens, não é necessário que se desenvolva uma
/Convolutional Neural Network/ do zero. Isso na verdade seria uma enorme perda
de tempo e recursos computacionais, além de exigir do desenvolvedor expertise em
tópicos como a inicialização dos pesos na rede neural. A técnica mais utilizada
hoje em dia para desenvolver esse tipo de aplicação é a utilização de /transfer
learning/, a modificação de redes neurais treinadas para outros fins que são
alteradas e passam por treinamento fino para executar outras atividades.

Um exemplo comum é a utilização da DCNN ImageNet, uma rede neural com 60 milhões
de parametros e 500 mil neurons treinada com mais de 1,3 milhão de imagens para
a classificação de 1000 diferentes classes. A idéia, nesse caso, é remover a
última camada da rede, treinada para a classificação das 1000 classes, e
substituir por outra que será treinada com um novo dataset para um novo objetivo.

*Remover usando o index*
#+BEGIN_SRC python
resnet18 = models.resnet18()

cnn_modules = list(resnet18.children())[:-1]

cnn = nn.Sequential(*cnn_modules)

#+END_SRC

*Remover definindo ponto de corte*
#+BEGIN_SRC python
resnet18 = models.resnet18()

cut = next(i for i,o in enumerate(resnet18.children()) if isinstance(o,nn.AdaptiveAvgPool2d))
m_cut = resnet18[:cut]

cnn = nn.Sequential(m_cut, AdaptiveConcatPool2d())

#+END_SRC
* Best command line tools
:PROPERTIES:
:EXPORT_DATE: 2020-02-10
:EXPORT_FILE_NAME: awesome-command-line-tools
:END:
** [[https://github.com/Canop/broot][broot]]
#+BEGIN_SRC sh
br -dp # replace 'ls', display dates and permissions
br -s  # to identify what is taking disk space
:gf    # display git statuses

#+END_SRC
** [[https://github.com/clvv/fasd][fasd]]
#+BEGIN_SRC sh
alias a='fasd -a'        # any
alias s='fasd -si'       # show / search / select
alias d='fasd -d'        # directory
alias f='fasd -f'        # file
alias sd='fasd -sid'     # interactive directory selection
alias sf='fasd -sif'     # interactive file selection
alias z='fasd_cd -d'     # cd, same functionality as j in autojump
alias zz='fasd_cd -d -i' # cd with interactive selection
alias v='f -e vim'       # quick opening files with vim
#+END_SRC
** [[https://github.com/sharkdp/fd][fd]]
#+BEGIN_SRC sh
# Convert all jpg files to png files:
fd -e jpg -x convert {} {.}.png

fd passwd /etc   # Searching 'passwd' in the /etc folder
fd -e md         # Searching for a particular file extension
fd -H pre-commit # Include hidden files
fd -E '*.bak'    # Excluding  specific files or directories

#+END_SRC

** [[HTTPS://github.com/junegunn/fzf][fzf]]
#+BEGIN_SRC sh
vim **<TAB>       # Select multiple items with TAB key
fd --type f | fzf # Feed the output of fd into fzf

# Setting fd as the default source for fzf
export FZF_DEFAULT_COMMAND='fd --type f'

#+END_SRC
** [[https://github.com/BurntSushi/ripgrep][ripgrep]]
#+BEGIN_SRC sh
rg 'fast\w+' README.md
#+END_SRC
* Vim Personal Minimal Setup
:PROPERTIES:
:EXPORT_FILE_NAME: vim-minimal-setup
:EXPORT_DATE: 2020-02-10
:END:

Use Vim without your own .vimrc is not an easy task. Tranning to a
certification test I was forced to use an virtual machine without my
customizations. So, I had to find an minimal setup that would make the default
Vim usable for me. That one linner is what I type every time I open Vim
without my config:

#+BEGIN_SRC sh
set nocp rnu ai et sw=4 ts=4 bs=2
#+END_SRC

To start Vim without loading your configs:

#+BEGIN_SRC sh
vim -u NONE
#+END_SRC

Each config option explained:

#+BEGIN_SRC sh
set nocp # nocompatible
set nu   # number
set rnu  # relativenumber
set et   # expandtab
set ai   # autoindent "automatic indentation
set ts=4 # tabstop
set sw=4 # shiftwidth
set bs=2 # backspace "allows the backspace to work
#+END_SRC
* My Local Hadoop/Spark Config files
** mapred-site.xml

#+BEGIN_SRC xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
#+END_SRC

** hdfs-site.xml

#+BEGIN_SRC xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.name.dir</name>
    <value>file:///home/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.data.dir</name>
    <value>file:///home/hadoop/hdfs/datanode</value>
  </property>
  <property>
    <name>dfs.block.size</name>
    <value>67108864</value>
    <description>Block size</description>
  </property>
</configuration>
#+END_SRC

** .bashrc

#+BEGIN_SRC sh

# JDK
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin

# Spark
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=notebook
export PYSPARK_PYTHON=/opt/anaconda/anaconda3/envs/ml/bin/python

# Hadoop
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export YARN_HOME=$HADOOP_HOME

# Sqoop
export SQOOP_HOME=/opt/sqoop
export PATH=$PATH:$SQOOP_HOME/bin

#+END_SRC
